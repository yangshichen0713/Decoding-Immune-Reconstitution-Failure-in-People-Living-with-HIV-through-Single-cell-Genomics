{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set ok!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import f1_score\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from scipy import sparse\n",
    "from module import *\n",
    "\n",
    "def setup_seed(seed):\n",
    "\n",
    "    np.random.seed(seed) \n",
    "    random.seed(seed)\n",
    "    \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  \n",
    "    \n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    torch.backends.cudnn.enabled = False  \n",
    "    torch.backends.cudnn.benchmark = False  \n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    print(\"seed set ok!\")\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "setup_seed(3407)\n",
    "\n",
    "import pickle\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "    \n",
    "# 定义一个函数，用于加载文件中的数据\n",
    "# 定义一个函数，用于加载文件中的数据\n",
    "    # 打开文件，以二进制模式读取\n",
    "def load_data(filename):\n",
    "        # 使用pickle模块加载文件中的数据\n",
    "    with open(filename, 'rb') as f:\n",
    "    # 返回加载的数据\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class scDataset(Dataset):\n",
    "    def __init__(self,index,mode):\n",
    "        # 初始化函数，传入index和mode\n",
    "        self.path='/home/share/huadjyin/home/zhouxuanchi/HIV/atac_to_gene_new_data_0218/adata_process'\n",
    "        self.stage=np.load('/home/share/huadjyin/home/zhouxuanchi/HIV/new_atac_and_gene_to_class/HDs_INRs/data/stage.npy')\n",
    "        self.index_list=index\n",
    "        self.mode=mode\n",
    "    def __len__(self):\n",
    "        return len(self.index_list)\n",
    "    def get_np_array(self, filename):\n",
    "        return np.load(os.path.join(self.path, filename))\n",
    "    def __getitem__(self, idx):\n",
    "        index_name=self.index_list[idx]\n",
    "        array_idx=self.get_np_array(str(index_name)+'.npy')\n",
    "        \n",
    "        tensor_all=torch.tensor(array_idx, dtype=torch.bfloat16)\n",
    "        mask=torch.tensor((tensor_all[:582] != 0), dtype=torch.bfloat16)\n",
    "        gene = tensor_all[:582]\n",
    "        peak = tensor_all[582:]\n",
    "        label=self.stage[index_name]\n",
    "        if self.mode=='hds_or_inrs':\n",
    "            if label=='HDs':\n",
    "                label=torch.tensor(0,dtype=torch.float32)\n",
    "            else:\n",
    "                label=torch.tensor(1,dtype=torch.float32)\n",
    "        elif self.mode=='hds_or_irs':\n",
    "            if label=='HDs':\n",
    "                label=torch.tensor(0,dtype=torch.float32)\n",
    "            else:\n",
    "                label=torch.tensor(1,dtype=torch.float32)\n",
    "        elif self.mode=='irs_or_inrs':\n",
    "            if label=='IRs':\n",
    "                label=torch.tensor(0,dtype=torch.float32)\n",
    "            else:\n",
    "                label=torch.tensor(1,dtype=torch.float32)\n",
    "        else:\n",
    "            return  ValueError(\"some error.\")\n",
    "        return gene, peak,mask,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sample_stage=load_data('./data/dict_sample_stage.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_list=['HD-H162','HD-H323','HD-H330','HD-H150','HD-H325','PD-H292','PD-H262','PD-H296','PD-H279','PD-H297','PD-H263','PD-H232','PD-H230','PD-H237','PD-H233']\n",
    "hds_test=test_list[0:5]\n",
    "inrs_test=test_list[10:15]\n",
    "irs_test=test_list[5:10]\n",
    "mode='hds_or_inrs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HIVModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim=64, hidden_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gene_encoder = TokenizedFAEncoder(582, 64, True, 7, 0.1, 'layernorm')\n",
    "        self.peak_encoder = TokenizedFAEncoder(5583, 64, True, 7, 0.1, 'layernorm')\n",
    "        self.decoder = GatedMLP(in_features=2*64, out_features=1)\n",
    "        self.matrix=torch.load('/home/share/huadjyin/home/zhouxuanchi/HIV/atac_to_gene_new_data_0218/data/mask_mat.pt')\n",
    "    def forward(self, gene, peak,mask_gene):   \n",
    "        matrix=torch.tensor(self.matrix,device=mask_gene.device)\n",
    "        mask_peak=torch.mm(mask_gene,matrix)\n",
    "        mask_gene = torch.cat((torch.zeros(size=(mask_gene.shape[0],1), dtype=torch.bfloat16,device=mask_gene.device), mask_gene), dim=1)\n",
    "        mask_peak = torch.cat((torch.zeros(size=(mask_peak.shape[0],1), dtype=torch.bfloat16,device=mask_gene.device), mask_peak), dim=1)\n",
    "        #[B, 582] -> [B, 583, 64]\n",
    "        gene = self.gene_encoder(gene,mask_gene)\n",
    "        #[B, 5583] -> [B, 5584, 64]\n",
    "        peak = self.peak_encoder(peak,mask_peak)\n",
    "        \n",
    "        if mask_gene is not None:\n",
    "        \n",
    "            m = mask_gene.unsqueeze(-1).float()\n",
    "            gene = (gene * m).sum(1) / m.sum(1)  \n",
    "        if mask_peak is not None:\n",
    "            m = mask_peak.unsqueeze(-1).float()\n",
    "            peak = (peak * m).sum(1) / m.sum(1)\n",
    "        x = torch.cat((gene, peak), dim=1)\n",
    "        # [B, 64] -> [B, 1] -> [B]\n",
    "        x = self.decoder(x).squeeze()\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        gene, peak,mask,label= batch\n",
    "        label_stage = label.view(-1)\n",
    "\n",
    "        pred_stage = self(gene, peak,mask)\n",
    "        label_stage=label_stage.squeeze(dim=-1)\n",
    "        loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))(pred_stage, label_stage)\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        gene, peak,mask,label= batch\n",
    "        \n",
    "        label_stage = label.view(-1)\n",
    "\n",
    "        pred_stage = self(gene, peak,mask)\n",
    "        label_stage=label_stage.squeeze(dim=-1)\n",
    "        loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))(pred_stage, label_stage)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        gene, peak,mask,label= batch\n",
    "        label_stage = label.unsqueeze(1).float()\n",
    "        pred_stage = self(gene, peak,mask)\n",
    "\n",
    "\n",
    "        return label_stage, pred_stage\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        step_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.9)\n",
    "        optim_dict = {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': step_lr_scheduler,\n",
    "                'monitor': 'val_loss',\n",
    "            }\n",
    "        }\n",
    "        return optim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/share/huadjyin/home/zhouxuanchi/.conda/envs/hiv_cxj/lib/python3.10/site-packages/flash_attn/ops/triton/layer_norm.py:959: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/home/share/huadjyin/home/zhouxuanchi/.conda/envs/hiv_cxj/lib/python3.10/site-packages/flash_attn/ops/triton/layer_norm.py:1018: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout, *args):\n",
      "/tmp/ipykernel_3101499/1822946953.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://githubfast.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.matrix=torch.load('/home/share/huadjyin/home/zhouxuanchi/HIV/atac_to_gene_new_data_0218/data/mask_mat.pt')\n"
     ]
    }
   ],
   "source": [
    "model = HIVModel.load_from_checkpoint(\"./model/hiv_model-epoch=37-val_loss=0.2243.ckpt\",map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3101499/1681762581.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "  0%|          | 0/2655 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m ig \u001b[38;5;241m=\u001b[39m IntegratedGradients(model)\n\u001b[1;32m     32\u001b[0m dict_sample\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgene\u001b[39m\u001b[38;5;124m'\u001b[39m:[],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeak\u001b[39m\u001b[38;5;124m'\u001b[39m:[]}\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader): \n\u001b[1;32m     34\u001b[0m     gene, peak,mask,label\u001b[38;5;241m=\u001b[39mi\n\u001b[1;32m     35\u001b[0m     gene \u001b[38;5;241m=\u001b[39m gene\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/hiv_cxj/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/hiv_cxj/lib/python3.10/site-packages/torch/utils/data/dataloader.py:440\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/hiv_cxj/lib/python3.10/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/hiv_cxj/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1038\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1031\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/.conda/envs/hiv_cxj/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/hiv_cxj/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/hiv_cxj/lib/python3.10/multiprocessing/context.py:281\u001b[0m, in \u001b[0;36mForkProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_fork\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/hiv_cxj/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/hiv_cxj/lib/python3.10/multiprocessing/popen_fork.py:66\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m parent_r, child_w \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpipe()\n\u001b[1;32m     65\u001b[0m child_r, parent_w \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpipe()\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "device='cuda:1'\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from captum.attr import IntegratedGradients\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "    \n",
    "# 定义一个函数，用于加载文件中的数据\n",
    "# 定义一个函数，用于加载文件中的数据\n",
    "    # 打开文件，以二进制模式读取\n",
    "def load_data(filename):\n",
    "        # 使用pickle模块加载文件中的数据\n",
    "    with open(filename, 'rb') as f:\n",
    "    # 返回加载的数据\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "dict_output={}\n",
    "##################\n",
    "sample_list=hds_test+inrs_test\n",
    "###################\n",
    "dict_sample_cell=load_data('./data/dict_sample_cell.pkl')\n",
    "with torch.cuda.amp.autocast():\n",
    "    for test_sample in sample_list:\n",
    "        idx_list=dict_sample_cell[test_sample]\n",
    "        dataset_test=scDataset(idx_list,mode)\n",
    "        \n",
    "        test_loader = DataLoader(dataset_test, batch_size=8, shuffle=False, num_workers=8)\n",
    "        model=model.to(device)\n",
    "        ig = IntegratedGradients(model)\n",
    "        dict_sample={'gene':[],'peak':[]}\n",
    "        for i in tqdm(test_loader): \n",
    "            gene, peak,mask,label=i\n",
    "            gene = gene.to(device)\n",
    "            peak = peak.to(device)\n",
    "            mask = mask.to(device)\n",
    "            attributions = ig.attribute((gene,peak), additional_forward_args=mask,n_steps=50,baselines=(0.0,0.0))\n",
    "            dict_sample['gene'].append(attributions[0].cpu().detach().numpy())\n",
    "            dict_sample['peak'].append(attributions[1].cpu().detach().numpy())\n",
    "            del peak,mask,attributions\n",
    "            break\n",
    "        del model\n",
    "        save_data(dict_sample,'/home/share/huadjyin/home/zhouxuanchi/HIV/final_chance/HDs_INRs_IG/output/'+str(test_sample)+'.pkl')\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiv_cxj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
